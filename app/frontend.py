import streamlit as st
import pandas as pd
import altair as alt
import requests
import os
import random

BACKEND_URL = os.getenv("BACKEND_URL", "http://localhost:8000/predict")

# Page Config
st.set_page_config(
    page_title="Sentinelp: AI Content Moderation System",
    page_icon="üõ°Ô∏è",
    layout="wide"
)

# Main Title
st.title("üõ°Ô∏è Sentinelp: AI Content Moderation System")

# Sidebar - About Section
st.sidebar.header("About")
st.sidebar.markdown("""
This system evaluates content toxicity using three different models:
- **TF-IDF**: Baseline model.
- **Bi-LSTM**: Recurrent Neural Network.
- **RoBERTa**: Transformer-based model (State of the Art).
""")

# Tabs
tab1, tab2, tab3 = st.tabs(["Live Scanner", "Model Benchmarks", "Reliability Lab"])

with tab1:
    st.header("Live Scanner")
    
    # Input Area
    user_input = st.text_area("Enter text to analyze", placeholder="Type your comment here...", height=150)
    
    if st.button("Scan", type="primary"):
        if not user_input.strip():
            st.warning("Please enter some text to scan.")
        else:
            with st.spinner("Scanning content..."):
                try:
                    response = requests.post(BACKEND_URL, json={"text": user_input}, timeout=5)
                    if response.status_code == 200:
                        data = response.json()
                        flags = data.get("flags", [])
                        scores = data.get("scores", {})
                        
                        # Display Flags
                        if not flags:
                            st.success("‚úÖ **Content Safe**")
                        else:
                            st.error(f"üö® **Toxic Content Detected**: {', '.join(flags)}")
                            
                        # Display Scores
                        st.subheader("Detailed Probabilities")
                        score_df = pd.DataFrame(list(scores.items()), columns=["Class", "Probability"])
                        st.bar_chart(score_df.set_index("Class"))
                        
                    else:
                        st.error(f"API Error: {response.text}")
                except requests.exceptions.ConnectionError:
                    st.error("Error: Could not connect to backend. Ensure the API is running.")
                except Exception as e:
                    st.error(f"An unexpected error occurred: {e}")

with tab2:
    st.header("Model Benchmarks")
    
    # Hardcoded Data
    data = {
        "Model": ["TF-IDF (Baseline)", "Bi-LSTM", "RoBERTa (Fine-Tuned)"],
        "F1_Score": [0.54, 0.53, 0.47],
        "Recall_Safety": [0.83, 0.84, 0.94],
        "Latency_ms": [5, 15, 80],
        "Size_MB": [50, 200, 500]
    }
    df_bench = pd.DataFrame(data)
    
    # Chart
    st.subheader("Latency vs. Accuracy Trade-off")
    
    chart = alt.Chart(df_bench).mark_circle().encode(
        x=alt.X('Latency_ms', title='Latency (ms)'),
        y=alt.Y('Recall_Safety', title='Safety Recall', scale=alt.Scale(zero=False)),
        size=alt.Size('Size_MB', title='Model Size (MB)', scale=alt.Scale(range=[100, 1000])),
        color='Model',
        tooltip=['Model', 'F1_Score', 'Recall_Safety', 'Latency_ms', 'Size_MB']
    ).properties(
        width=700,
        height=400
    ).interactive()
    
    st.altair_chart(chart, use_container_width=True)
    
    st.info("üí° ** Insight**: RoBERTa offers the highest safety recall (94%) but at 16x higher latency than the baseline. Bi-LSTM offers a balanced middle ground.")

with tab3:
    st.header("Reliability Lab")
    
    # Helper for noise injection
    def inject_noise(text):
        if len(text) < 2:
            return text
        chars = list(text)
        # Randomly pick a position to swap
        idx = random.randint(0, len(chars) - 2)
        chars[idx], chars[idx+1] = chars[idx+1], chars[idx]
        return "".join(chars)
    
    # UI Components
    test_text = st.text_input("Test String", value="You are a huge idiot")
    add_noise = st.checkbox("Inject Noise (Typos)")
    
    final_text = test_text
    if add_noise:
        final_text = inject_noise(test_text)
        st.markdown(f"**Noisy Text**: `{final_text}`")
        
    if st.button("Run Robustness Test"):
        with st.spinner("Testing robustness..."):
            try:
                response = requests.post(BACKEND_URL, json={"text": final_text}, timeout=5)
                if response.status_code == 200:
                    data = response.json()
                    flags = data.get("flags", [])
                    scores = data.get("scores", {})
                    
                    if not flags:
                        st.success(f"‚úÖ Safe (Score: {scores.get('toxic', 0):.2f})")
                    else:
                        st.error(f"üö® Toxic Analysis: {', '.join(flags)} (Score: {scores.get('toxic', 0):.2f})")
                        
                    st.bar_chart(pd.DataFrame(list(scores.items()), columns=["Class", "Probability"]).set_index("Class"))

                else:
                    st.error(f"API Error: {response.text}")
            except Exception as e:
                st.error(f"Connection Error: {e}")
                
    st.markdown("---")
    st.info("üìä **Robustness Note**: In benchmark tests, RoBERTa maintained performance with 10% noise, whereas TF-IDF performance dropped by 15%.")
